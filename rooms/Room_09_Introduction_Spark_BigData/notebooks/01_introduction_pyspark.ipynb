{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction à PySpark\n",
                "\n",
                "Dans ce notebook, nous allons découvrir les bases de Spark via l'API PySpark. Nous allons voir comment initialiser une session, créer des DataFrames, et effectuer des transformations de base."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialisation de la SparkSession\n",
                "\n",
                "Le point d'entrée de toute application Spark est la `SparkSession`. C'est elle qui gère la connexion au cluster (ou au mode local dans notre cas)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "\n",
                "# Création de la session Spark\n",
                "# 'local[*]' signifie utiliser tous les coeurs du processeur disponible\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"Introduction PySpark\") \\\n",
                "    .master(\"local[*]\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(\"Spark Version:\", spark.version)\n",
                "spark"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Création de DataFrames\n",
                "\n",
                "Spark utilise des DataFrames, similaires à Pandas, mais distribués. Créons un DataFrame simple à partir d'une liste."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Données : Liste de tuples (Nom, Âge, Département)\n",
                "data = [\n",
                "    (\"Alice\", 34, \"Marketing\"),\n",
                "    (\"Bob\", 45, \"IT\"),\n",
                "    (\"Charlie\", 29, \"Marketing\"),\n",
                "    (\"David\", 38, \"Finance\"),\n",
                "    (\"Eva\", 23, \"IT\")\n",
                "]\n",
                "\n",
                "# Schéma : Noms des colonnes\n",
                "columns = [\"Nom\", \"Age\", \"Departement\"]\n",
                "\n",
                "# Création du DataFrame\n",
                "df = spark.createDataFrame(data, schema=columns)\n",
                "\n",
                "# Affichage du schéma et des données\n",
                "df.printSchema()\n",
                "df.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Transformations vs Actions\n",
                "\n",
                "Spark utilise l'évaluation paresseuse (**Lazy Evaluation**). \n",
                "- Une **Transformation** (ex: `filter`, `select`) ne déclenche aucun calcul, elle crée juste un nouveau plan d'exécution.\n",
                "- Une **Action** (ex: `show`, `count`, `collect`) déclenche le calcul réel.\n",
                "\n",
                "Observez ci-dessous :"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transformation : Filtrer les employés de plus de 30 ans\n",
                "# Rien ne se passe ici au niveau du cluster, Spark prépare juste le plan\n",
                "df_filtered = df.filter(df[\"Age\"] > 30)\n",
                "\n",
                "# Action : Compter les résultats\n",
                "# Ici, le calcul est lancé\n",
                "count = df_filtered.count()\n",
                "print(f\"Nombre d'employés > 30 ans : {count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Opérations d'agrégation\n",
                "\n",
                "Comme en SQL, on peut grouper et agréger des données."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# A COMPLETER : Calculer la moyenne d'âge par département\n",
                "# Utilisez groupBy() et agg()\n",
                "\n",
                "# df_avg_age = ...\n",
                "# df_avg_age.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion provisoire\n",
                "N'oubliez jamais de fermer votre session Spark à la fin du traitement pour libérer les ressources."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}