{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deep Dive Spark SQL & Window Functions\n",
                "\n",
                "Ce notebook est destiné à vous faire passer au niveau supérieur. Les **Window Functions** sont l'outil le plus puissant pour l'analyse de séries temporelles et de classements en SQL/Spark."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.window import Window\n",
                "from pyspark.sql.functions import col, row_number, rank, lag, lead, desc, sum as _sum\n",
                "\n",
                "spark = SparkSession.builder.appName(\"Spark SQL Deep Dive\").master(\"local[*]\").getOrCreate()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Window Functions (Fonctions de Fenêtrage)\n",
                "\n",
                "Imaginez que vous voulez comparer le salaire d'un employé avec la moyenne de SON département, ou voir l'évolution des ventes d'un jour à l'autre.\n",
                "\n",
                "On définit une \"Fenêtre\" (Window) qui partitionne les données."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Données : Ventes par Vendeur et par Jour\n",
                "data_ventes = [\n",
                "    (\"2023-01-01\", \"Alice\", 100),\n",
                "    (\"2023-01-02\", \"Alice\", 150),\n",
                "    (\"2023-01-03\", \"Alice\", 120),\n",
                "    (\"2023-01-01\", \"Bob\", 200),\n",
                "    (\"2023-01-02\", \"Bob\", 180),\n",
                "    (\"2023-01-03\", \"Bob\", 250)\n",
                "]\n",
                "df_ventes = spark.createDataFrame(data_ventes, [\"date\", \"vendeur\", \"montant\"])\n",
                "\n",
                "# Objectif : Calculer la différence de vente par rapport à la veille pour CHAQUE vendeur"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Définir la Fenêtre : On partitionne par Vendeur et on trie par Date\n",
                "windowSpec = Window.partitionBy(\"vendeur\").orderBy(\"date\")\n",
                "\n",
                "# 2. Utiliser LAG (valeur précédente)\n",
                "df_lag = df_ventes.withColumn(\"montant_veille\", lag(\"montant\", 1).over(windowSpec))\n",
                "\n",
                "# 3. Calculer la différence\n",
                "df_diff = df_lag.withColumn(\"progression\", col(\"montant\") - col(\"montant_veille\"))\n",
                "\n",
                "df_diff.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Classements (Rank vs Dense Rank)\n",
                "\n",
                "Établir un classement des meilleurs vendeurs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "windowRank = Window.orderBy(desc(\"montant\"))\n",
                "\n",
                "df_rank = df_ventes.withColumn(\"classement_global\", rank().over(windowRank))\n",
                "df_rank.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Complexes Types (Arrays & Maps)\n",
                "\n",
                "Spark gère très bien les données imbriquées (comme du JSON)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import explode, split\n",
                "\n",
                "# Données : Un livre et une liste de tags (sous forme de chaîne)\n",
                "data_books = [\n",
                "    (\"Harry Potter\", \"magie,jeunesse,fantasy\"),\n",
                "    (\"Le Seigneur des Anneaux\", \"fantasy,aventure,epique\")\n",
                "]\n",
                "df_books = spark.createDataFrame(data_books, [\"titre\", \"tags_str\"])\n",
                "\n",
                "# Convertir la chaîne en Array\n",
                "df_arrays = df_books.withColumn(\"tags_array\", split(col(\"tags_str\"), \",\"))\n",
                "\n",
                "# EXPLODE : Créer une ligne par tag (très utile !)\n",
                "df_exploded = df_arrays.select(\"titre\", explode(\"tags_array\").alias(\"tag_unique\"))\n",
                "\n",
                "df_exploded.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}