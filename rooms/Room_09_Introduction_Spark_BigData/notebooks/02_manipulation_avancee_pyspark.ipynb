{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Manipulation Avancée de Données avec PySpark\n",
                "\n",
                "Après avoir vu les bases, nous allons approfondir la manipulation de données. Ce notebook couvre les opérations essentielles que vous utiliserez dans 90% de vos projets Data Engineering : lecture/écriture, nettoyage, et logique conditionnelle."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialisation et Chargement de Données\n",
                "\n",
                "Nous commençons par initialiser la session Spark."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import col, lit, when, avg, count\n",
                "\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"Manipulation Avancée\") \\\n",
                "    .master(\"local[*]\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "# Création d'un jeu de données complexe pour l'exercice\n",
                "data = [\n",
                "    (1, \"Martin\", \"Paris\", 3500, \"2023-01-01\", None),\n",
                "    (2, \"Sophie\", \"Lyon\", 4200, \"2023-02-15\", \"Manager\"),\n",
                "    (3, \"Paul\", \"Paris\", 3100, \"2023-03-10\", None),\n",
                "    (4, \"Julie\", \"Marseille\", 5000, \"2022-05-20\", \"Director\"),\n",
                "    (5, \"Antoine\", \"Lyon\", 2800, None, \"Junior\"),\n",
                "    (6, None, \"Paris\", 3200, \"2023-06-01\", \"Junior\")\n",
                "]\n",
                "\n",
                "schema = [\"id\", \"nom\", \"ville\", \"salaire\", \"date_embauche\", \"titre\"]\n",
                "\n",
                "df = spark.createDataFrame(data, schema=schema)\n",
                "df.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Opérations sur les Colonnes\n",
                "\n",
                "En PySpark, on modifie rarement un DataFrame \"en place\". On crée de nouvelles transformations.\n",
                "\n",
                "### Ajouter et Renommer des colonnes\n",
                "Utilisez `.withColumn()` pour ajouter ou modifier une colonne, et `.withColumnRenamed()` pour renommer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ajouter une colonne 'salaire_annuel' (salaire * 12)\n",
                "df_calcul = df.withColumn(\"salaire_annuel\", col(\"salaire\") * 12)\n",
                "\n",
                "# Ajouter une colonne constante 'pays'\n",
                "df_calcul = df_calcul.withColumn(\"pays\", lit(\"France\"))\n",
                "\n",
                "# Renommer 'nom' en 'prenom'\n",
                "df_calcul = df_calcul.withColumnRenamed(\"nom\", \"prenom\")\n",
                "\n",
                "df_calcul.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Logique Conditionnelle (Case When)\n",
                "L'équivalent de `IF / ELSE` ou `CASE WHEN` en SQL se fait avec `when().otherwise()`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Créer une catégorie de salaire\n",
                "df_categorie = df.withColumn(\"categorie_salaire\", \n",
                "    when(col(\"salaire\") > 4000, \"Élevé\")\n",
                "    .when(col(\"salaire\") > 3000, \"Moyen\")\n",
                "    .otherwise(\"Faible\")\n",
                ")\n",
                "\n",
                "df_categorie.select(\"nom\", \"salaire\", \"categorie_salaire\").show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Gestion des Valeurs Nulles (Missing Values)\n",
                "\n",
                "Les données réelles contiennent souvent des `null`. Spark propose `fillna` et `dropna`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Remplacer les Nulls dans 'titre' par 'Inconnu' et dans 'nom' par 'Anonyme'\n",
                "df_clean = df.na.fill({\n",
                "    \"titre\": \"Inconnu\",\n",
                "    \"nom\": \"Anonyme\"\n",
                "})\n",
                "\n",
                "# Supprimer les lignes où 'date_embauche' est null\n",
                "df_clean = df_clean.na.drop(subset=[\"date_embauche\"])\n",
                "\n",
                "df_clean.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Utilisation du SQL natif\n",
                "\n",
                "Une des grandes forces de Spark est sa compatibilité SQL. Vous pouvez transformer n'importe quel DataFrame en \"Table Temporaire\" et écrire du SQL standard."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Création de la vue temporaire\n",
                "df.createOrReplaceTempView(\"employes\")\n",
                "\n",
                "# Requête SQL\n",
                "resultat_sql = spark.sql(\"\"\"\n",
                "    SELECT ville, AVG(salaire) as salaire_moyen, COUNT(*) as nb_employes\n",
                "    FROM employes\n",
                "    GROUP BY ville\n",
                "    HAVING AVG(salaire) > 3000\n",
                "\"\"\")\n",
                "\n",
                "resultat_sql.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Lecture et Écriture (I/O)\n",
                "\n",
                "En production, on n'utilise pas `createDataFrame`. On lit des fichiers (CSV, Parquet, JSON)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Écriture au format Parquet (Format colonnaire compressé, standard Big Data)\n",
                "# mode(\"overwrite\") permet d'écraser le dossier s'il existe déjà\n",
                "df.write.mode(\"overwrite\").parquet(\"data_output/employes.parquet\")\n",
                "\n",
                "# Lecture du fichier Parquet\n",
                "df_parquet = spark.read.parquet(\"data_output/employes.parquet\")\n",
                "df_parquet.printSchema()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": [],
            "outputs": [],
            "source": [
                "spark.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}