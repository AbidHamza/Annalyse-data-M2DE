{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Manipulation Avancée de Données avec PySpark\n",
                "\n",
                "Ce notebook est le \"Couteau Suisse\" du Data Engineer. Nous allons voir comment aller au-delà des simples filtres pour réaliser des opérations complexes : Jointures, UDFs (Fonctions Utilisateurs), et Optimisation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import col, lit, when, udf, avg, count, desc\n",
                "from pyspark.sql.types import StringType, IntegerType\n",
                "\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"Manipulation Avancée Expert\") \\\n",
                "    .master(\"local[*]\") \\\n",
                "    .getOrCreate()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Création de Données Relationnelles (Pour les Jointures)\n",
                "\n",
                "Pour comprendre les jointures, il nous faut deux tables : `Employes` et `Departements`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Table Employés\n",
                "data_emp = [\n",
                "    (1, \"Martin\", 10),\n",
                "    (2, \"Sophie\", 10),\n",
                "    (3, \"Paul\", 20),\n",
                "    (4, \"Julie\", 30),\n",
                "    (5, \"Fantôme\", 99) # Dept 99 n'existe pas\n",
                "]\n",
                "cols_emp = [\"id\", \"nom\", \"dept_id\"]\n",
                "df_emp = spark.createDataFrame(data_emp, cols_emp)\n",
                "\n",
                "# Table Départements\n",
                "data_dept = [\n",
                "    (10, \"Marketing\", \"Paris\"),\n",
                "    (20, \"Finance\", \"Lyon\"),\n",
                "    (30, \"IT\", \"Bordeaux\"),\n",
                "    (40, \"RH\", \"Lille\") # Aucun employé ici\n",
                "]\n",
                "cols_dept = [\"id_dept\", \"nom_dept\", \"ville\"]\n",
                "df_dept = spark.createDataFrame(data_dept, cols_dept)\n",
                "\n",
                "print(\"--- Employés ---\")\n",
                "df_emp.show()\n",
                "print(\"--- Départements ---\")\n",
                "df_dept.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Les Jointures (Joins)\n",
                "\n",
                "C'est le concept le plus important en base de données relationnelle.\n",
                "\n",
                "- **Inner Join** : Ne garde que ce qui matche des deux côtés.\n",
                "- **Left Join** : Garde TOUT ce qui est à gauche (Employés), même s'il n'y a pas de département.\n",
                "- **Right Join** : Garde TOUT ce qui est à droite (Départements), même s'il n'y a pas d'employé."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inner Join : Martin, Sophie, Paul, Julie (Fantôme disparaît, RH disparaît)\n",
                "print(\"--- Inner Join ---\")\n",
                "df_emp.join(df_dept, df_emp.dept_id == df_dept.id_dept, \"inner\").show()\n",
                "\n",
                "# Left Join : Fantôme apparait avec des NULLs pour le département\n",
                "print(\"--- Left Join ---\")\n",
                "df_emp.join(df_dept, df_emp.dept_id == df_dept.id_dept, \"left\").show()\n",
                "\n",
                "# Full Outer Join : Tout le monde est là\n",
                "print(\"--- Full Outer Join ---\")\n",
                "df_emp.join(df_dept, df_emp.dept_id == df_dept.id_dept, \"outer\").show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. User Defined Functions (UDFs)\n",
                "\n",
                "Parfois, les fonctions natives de Spark ne suffisent pas. Vous voulez appliquer votre propre fonction Python.\n",
                "\n",
                "**Attention** : Les UDFs sont plus lentes que les fonctions natives car Spark doit sérialiser les données vers Python et revenir. À utiliser avec parcimonie."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Définition d'une fonction Python classique\n",
                "def categoriser_salaire_complexe(nom, id):\n",
                "    if len(nom) > 5 and id % 2 == 0:\n",
                "        return \"Elite\"\n",
                "    else:\n",
                "        return \"Standard\"\n",
                "\n",
                "# Conversion en UDF Spark (Nécessite de préciser le type de retour)\n",
                "udf_salaire = udf(categoriser_salaire_complexe, StringType())\n",
                "\n",
                "# Utilisation\n",
                "df_emp.withColumn(\"statut\", udf_salaire(col(\"nom\"), col(\"id\"))).show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Statistiques et Exploration\n",
                "\n",
                "Avant de se lancer dans le Machine Learning, il faut comprendre ses données."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistiques descriptives (count, mean, stddev, min, max)\n",
                "df_emp.describe().show()\n",
                "\n",
                "# Matrice de Corrélation (Approximative)\n",
                "print(\"Corrélation id vs dept_id :\", df_emp.stat.corr(\"id\", \"dept_id\"))\n",
                "\n",
                "# Tableaux croisés (Pivot Table)\n",
                "# Imaginons qu'on veuille compter les employés par département et par... (ici on n'a pas assez de colonnes variées, mais voici la syntaxe)\n",
                "df_emp.groupBy(\"dept_id\").count().show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Optimisation : Cache & Persist\n",
                "\n",
                "Si vous utilisez le même DataFrame plusieurs fois (par exemple pour l'entraîner dans un modèle, puis pour faire des stats), Spark va le recalculer DEPUIS LE DÉBUT à chaque fois.\n",
                "\n",
                "Pour éviter ça, on le met en cache (mémoire)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_gros_calcul = df_emp.join(df_dept, df_emp.dept_id == df_dept.id_dept, \"outer\")\n",
                "\n",
                "# Mise en cache -> La première action sera lente, les suivantes instantanées\n",
                "df_gros_calcul.cache()\n",
                "\n",
                "# Action 1 (Met en mémoire)\n",
                "print(\"Count 1 :\", df_gros_calcul.count())\n",
                "\n",
                "# Action 2 (Lit depuis la mémoire)\n",
                "print(\"Count 2 (Rapide) :\", df_gros_calcul.count())\n",
                "\n",
                "# Toujours vider le cache quand fini pour libérer la RAM\n",
                "df_gros_calcul.unpersist()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. SQL Avancé (CTE & Subqueries)\n",
                "\n",
                "On peut faire des requêtes très complexes avec des CTE (Common Table Expressions)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_emp.createOrReplaceTempView(\"emp\")\n",
                "df_dept.createOrReplaceTempView(\"dept\")\n",
                "\n",
                "spark.sql(\"\"\"\n",
                "    WITH DeptPops AS (\n",
                "        SELECT dept_id, COUNT(*) as nb_emp\n",
                "        FROM emp\n",
                "        GROUP BY dept_id\n",
                "    )\n",
                "    SELECT d.nom_dept, p.nb_emp\n",
                "    FROM dept d\n",
                "    JOIN DeptPops p ON d.id_dept = p.dept_id\n",
                "    WHERE p.nb_emp > 1\n",
                "\"\"\").show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}